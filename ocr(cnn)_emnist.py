# -*- coding: utf-8 -*-
"""OCR(CNN)_EMNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V9lfnNkkg3HEHdYOp9-C576-Lbi-Ik_j
"""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

!kaggle datasets download -d crawford/emnist

import zipfile
zip_ref = zipfile.ZipFile('/content/emnist.zip', 'r')
zip_ref.extractall('/content')
zip_ref.close()

import pandas as pd
train = pd.read_csv('emnist-balanced-train.csv')
test = pd.read_csv('emnist-balanced-test.csv')

columns=['labels']
for i in range(train.shape[1]-1):
  columns.append(i)
train.columns=columns
test.columns=columns  
classes=train['labels'].unique()
print('number of classes: ', len(classes))

# train.columns(will give list of column names)

# train.head()
test

from sklearn.model_selection import train_test_split

# split training and validation data using sklearn
x_train, x_val, y_train, y_val = train_test_split(train.drop(['labels'], axis=1),
                                                  train.labels,
                                                  train_size=0.8,
                                                  test_size=0.2,
                                                  random_state=42)

y_test=test['labels'].values

x_test=test.values[:,1:]

print('trianing set: ', x_train.shape, y_train.shape)
print('validation set: ', x_val.shape, y_val.shape)
print('test set: ', x_test.shape, y_test.shape)

# import random
# import matplotlib.pyplot as plt
# by_merge_map = {0:'0', 1:'1', 2:'2', 3:'3', 4:'4', 5:'5', 6:'6', 7:'7', 8:'8', 9:'9', 10:'A', 11:'B', 
#                 12:'C', 13:'D', 14:'E', 15:'F', 16:'G', 17:'H', 18:'I', 19:'J', 20:'K', 21:'L', 22:'M', 
#                 23:'N', 24:'O', 25:'P', 26:'Q', 27:'R', 28:'S', 29:'T', 30:'U', 31:'V', 32:'W', 33:'X', 
#                 34:'Y', 35:'Z', 36:'a', 37:'b', 38:'d', 39:'e', 40:'f', 41:'g', 42:'h', 43:'n', 44:'q', 
#                 45:'r', 46:'t'}
# train_samples = random.sample(range(0, len(x_train)), 9)
# test_samples = random.sample(range(0, len(x_val)), 9)

# plt.figure(figsize=(6, 6))
# plt.suptitle('Training set')
# for i in train_samples:
#     plt.subplot(3, 3, train_samples.index(i)+1)
#     plt.imshow(x_train.iloc[i,:].values.reshape(28,28), cmap='binary')
#     plt.title(f'label: {by_merge_map[y_train.iloc[i]]}')
#     plt.axis('off')
    
# plt.figure(figsize=(6, 6))
# plt.suptitle('Test set')
# for i in test_samples:
#     plt.subplot(3, 3, test_samples.index(i)+1)
#     plt.imshow(x_val.iloc[i,:].values.reshape(28,28), cmap='binary')
#     plt.title(f'label: {by_merge_map[y_val.iloc[i]]}')
#     plt.axis('off')

import tensorflow as tf
from tensorflow import keras
from keras import Sequential
from keras.layers import Flatten,Dense,Convolution2D,Reshape,MaxPooling2D,Dropout
from keras import layers

# model=Sequential()
# model.add(Reshape((28,28,1),input_shape=(784,)))
# model.add(Convolution2D(64,kernel_size=(3,3),padding='valid',activation='relu'))
# model.add(Convolution2D(64,kernel_size=(3,3),padding='valid',activation='relu'))
# model.add(layers.BatchNormalization())
# model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))
# model.add(Dropout(.3))

# model.add(Convolution2D(32,kernel_size=(3,3),padding='valid',activation='relu'))
# model.add(layers.BatchNormalization())
# model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))
# model.add(Dropout(.3))

# model.add(Convolution2D(16,kernel_size=(3,3),padding='valid',activation='relu'))
# model.add(layers.BatchNormalization())
# model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))
# model.add(Dropout(.3))

# model.add(Convolution2D(8,kernel_size=(1,1),padding='valid',activation='relu'))
# model.add(layers.BatchNormalization())


# model.add(Flatten())
# model.add(Dense(128,activation='relu'))
# model.add(Dropout(.3))
# model.add(Dense(len(classes),activation='softmax'))

model = tf.keras.models.Sequential([
        # initial normalization
        tf.keras.layers.Reshape((28, 28, 1), input_shape=(784,)),
#         tf.keras.layers.BatchNormalization(),
        
        # first convolution
        tf.keras.layers.Conv2D(8, (3, 3), activation='relu'), # applies kernels to our data
        tf.keras.layers.MaxPooling2D(2, 2), # reduce dimension
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.4),
        # second convolution
        tf.keras.layers.Conv2D(16, (3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(2, 2),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.4),
        
        # third convolution
        tf.keras.layers.Conv2D(24, (3, 3), activation='relu'),
#         tf.keras.layers.MaxPooling2D(2, 2),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.4),

    
        # feed to DNN
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation='relu'),
#         tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(len(classes), activation=tf.nn.softmax) # generalized logistic regression
    ])
    
model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

model.summary()

model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

# set accuracy
desired_accuracy = 0.92
    
# create callback to stop training when we reached desired accuracy
class myCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if(logs.get('accuracy') is not None and logs.get('accuracy') >= desired_accuracy):
            print('\nReached 92% training accuracy: cancelling training...')
            self.model.stop_training = True

callbacks = myCallback()

# history = model.fit(x_train, y_train, epochs=100,
#                     validation_data=(x_val, y_val),
#                     batch_size=4096, verbose=2,
#                     callbacks=[callbacks])

history = model.fit(x_train, y_train, epochs=1000,
                    validation_data=(x_val, y_val),
                    batch_size=4096, verbose=1,
                    callbacks=[callbacks])

# Plot training vs validation accruacy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('CNN Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Number of Epochs')
plt.legend(['training', 'validation'], loc='lower right')
plt.show()

# Plot training vs validation losses
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('CNN Model Loss')
plt.ylabel('Loss')
plt.xlabel('Number of Epochs')
plt.legend(['training', 'validation'], loc='upper right')
plt.show()



import numpy as np
test_samples = random.sample(range(0, len(x_test)), 30) # select 30 samples

# display results
plt.figure(figsize=(15, 6))
for i in test_samples:
    plt.subplot(3, 10, test_samples.index(i) + 1)  
    plt.imshow(x_test[i].reshape(28,28), cmap='binary')
    plt.title(f'predicted: {np.argmax(predictions[i])} \nactual: {y_test[i]}')
    plt.axis('off')



y_pred = model.predict(x_test)
y_pred_labels = np.argmax(y_pred, axis=1)

#model.evaluate(y_pred_labels,y_test)
y_test.dtype

# # Create the confusion matrix
# cm = metrics.confusion_matrix(y_test, y_pred_labels)

# # # Get the class labels
# # class_labels = []  
# # for i in range(48):
# #   class_labels.append(i)
# # class_labels  
# # import seaborn as sns

# # # Plot the confusion matrix
# # plt.figure(figsize=(8, 6))
# # sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
# # plt.xlabel('Predicted Labels')
# # plt.ylabel('True Labels')
# # plt.title('Confusion Matrix')
# # plt.show()

cm.shape

